# -*- coding: utf-8 -*-
"""MKCERegression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c7iXZMDDuTVsnhU6P62NST6t-z8e20K6

#Import Libraries and Load the dataset
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')
from sklearn import metrics
df = pd.read_excel("/content/EnergyEfficiency.xlsx")

"""**Dataset Information**"""

df.head()

"""**Rename the Features**"""

df.rename(columns={'X1': 'RelCompactness', 'X2': 'SurfaceArea','X3': 'WallArea', 'X4': 'RoofArea', 'X5': 'OverallHeight','X6': 'Orientation',
                   'X7': 'GlazingArea', 'X8': 'GlazingAreaDist','Y1': 'HeatingLoad', 'Y2': 'CoolingLoad'}, inplace=True)

df.head()

"""**Correlation of Features - Feature Selection**"""

import seaborn as sns

plt.figure(figsize=(12,10))
sns.heatmap(df.corr(),vmax=1, square=True, annot=True,cmap='coolwarm')
#plt.savefig('pearsoncorr', dpi=1080)

from pandas.plotting import scatter_matrix
pd.plotting.scatter_matrix(df[0:8],alpha=0.2,figsize = (12,8))

"""**Histogram of Feature - Distribution**"""

import seaborn as sns
plt.figure(figsize=(12,10))
sns.pairplot(df,diag_kind = 'kde',plot_kws = {'edgecolor': 'k'}, size = 4)
plt.show()

"""**Feature Engineering**"""

dfheat = pd.DataFrame(df, columns = df.columns)
dfheat.drop(['CoolingLoad'], axis=1, inplace=True)

dfheat.info()

dfcool = pd.DataFrame(df, columns = df.columns)
dfcool.drop(['HeatingLoad'], axis=1, inplace=True)

dfcool.info()

"""**Removal of Insignificant Features**"""

dfheat.drop(['SurfaceArea','RoofArea','Orientation'], axis=1, inplace=True)
dfheat.info()

"""**Data Splitting - Holdout Partition**"""

y = dfheat['HeatingLoad'].copy()
X = pd.DataFrame(dfheat, columns=dfheat.columns)
X.drop(['HeatingLoad'], axis=1, inplace=True)

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)

"""**Import Libraries for Machine Learning A;gorithms**"""

from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR

reg1=SVR(kernel='linear')
reg1.fit(X_train,y_train)

y_pred=reg1.predict(X_test)

"""**Statistical Measures**"""

from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error,r2_score
from sklearn.metrics import mean_squared_error

mae=mean_absolute_error(y_test,y_pred)
mae

r2err=r2_score(y_test,y_pred)
r2err

"""**Plotting of Results**"""

import matplotlib.pyplot as plt

plt.scatter(y_pred,y_test,color='blue')
plt.plot([y_test.min(),y_test.max()],[y_test.min(),y_test.max()],'r--',lw=2)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""**GRID SEARCH - Hyperparameter Tuning**"""

from sklearn.preprocessing import StandardScaler

from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
num_folds = 10
seed = 7
scoring = 'r2'

"""**Data Preprocessing - Z-Score Normalization**"""

scaler = StandardScaler().fit(X_train)
rescaledX = scaler.transform(X_train)

"""##KNN"""

k_values = np.array([1,3,5,7,9,11,13,15,17,19,21])
param_grid = dict(n_neighbors=k_values)
model = KNeighborsRegressor()
kfold = KFold(n_splits=3, random_state=42,shuffle=True)
grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)
grid_result = grid.fit(rescaledX, y_train)
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
  print("%f (%f) with: %r" % (mean, stdev, param))

model = KNeighborsRegressor(n_neighbors=1)
model.fit(X_train, y_train)
#Training Data
print("Training Results")
knnpredtr = model.predict(X_train)
print("MAE",mean_absolute_error(y_train, knnpredtr))
print("MSE",mean_squared_error(y_train, knnpredtr))
print("RMSE",np.sqrt(mean_squared_error(y_train, knnpredtr)))
r2 = r2_score(y_train, knnpredtr)
print("R-Square",r2)
#Testing Data
print("Testing Results")
knnpred = model.predict(X_test)
print("MAE",mean_absolute_error(y_test, knnpred))
print("MSE",mean_squared_error(y_test, knnpred))
print("RMSE",np.sqrt(mean_squared_error(y_test, knnpred)))
r2 = r2_score(y_test, knnpred)
print("R-Square",r2)

plt.scatter(y_test, knnpred,color='blue')
plt.plot([y_test.min(),y_test.max()],[y_test.min(),y_test.max()],'r--',lw=2)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""##SVM"""

cval= np.array([1.0,1.5,2,3,3.5,4.0])
epsval=np.array([0.5,1.0,2.0])
#kernelval=['rbf','linear']
#kernel='rbf', degree=3, gamma='scale', coef0=0.0, tol=0.001, C=1.0, epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=-1
param_grid = dict(C=cval,epsilon=epsval)
model = SVR()
kfold = KFold(n_splits=3, random_state=42,shuffle=True)
grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)
grid_result = grid.fit(rescaledX, y_train)
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
  print("%f (%f) with: %r" % (mean, stdev, param))

model = SVR(C= 4.0, epsilon= 1.0, kernel= 'linear')
model.fit(X_train, y_train)
#Training Data
print("Training Results")
svrpredtr = model.predict(X_train)
print("MAE",mean_absolute_error(y_train, svrpredtr))
print("MSE",mean_squared_error(y_train, svrpredtr))
print("RMSE",np.sqrt(mean_squared_error(y_train, svrpredtr)))
r2 = r2_score(y_train, svrpredtr)
print("R-Square",r2)
#Testing Data
print("Testing Results")
svrpred = model.predict(X_test)
print("MAE",mean_absolute_error(y_test, svrpred))
print("MSE",mean_squared_error(y_test, svrpred))
print("RMSE",np.sqrt(mean_squared_error(y_test, svrpred)))
r2 = r2_score(y_test, svrpred)
print("R-Square",r2)

plt.scatter(y_test, svrpred,color='blue')
plt.plot([y_test.min(),y_test.max()],[y_test.min(),y_test.max()],'r--',lw=2)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""**Random Forest**"""

from sklearn.ensemble import RandomForestRegressor

param_grid = {
    'n_estimators': range(20, 100, 20),
    'max_depth': range(2, 40, 2),
    'min_samples_split': range(2, 20, 2),
    'max_features': ["auto", "sqrt", "log2"],
}

model= RandomForestRegressor(random_state=42)
kfold = KFold(n_splits=3, random_state=42,shuffle=True)
grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)
grid_result = grid.fit(rescaledX, y_train)
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
  print("%f (%f) with: %r" % (mean, stdev, param))

model = RandomForestRegressor(max_depth=8, max_features='log2', min_samples_split=2, n_estimators= 80)
model.fit(X_train, y_train)
#Training Data
print("Training Results")
rfpredtr = model.predict(X_train)
print("MAE",mean_absolute_error(y_train, rfpredtr))
print("MSE",mean_squared_error(y_train, rfpredtr))
print("RMSE",np.sqrt(mean_squared_error(y_train, rfpredtr)))
r2 = r2_score(y_train, rfpredtr)
print("R-Square",r2)
#Testing Data
print("Testing Results")
rfpred = model.predict(X_test)
print("MAE",mean_absolute_error(y_test, rfpred))
print("MSE",mean_squared_error(y_test, rfpred))
print("RMSE",np.sqrt(mean_squared_error(y_test, rfpred)))
r2 = r2_score(y_test, rfpred)
print("R-Square",r2)

plt.scatter(y_test, svrpred,color='blue')
plt.plot([y_test.min(),y_test.max()],[y_test.min(),y_test.max()],'r--',lw=2)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()